{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter12 모델 선택\n",
    "\n",
    "## 12.0 소개\n",
    "\n",
    "## 12.1 완전 탐색을 사용해 최선의 모델 선택하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 GridSearchCV 활용\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 데이터 로드\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 로지스틱 회귀 모델 제작\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# 패널티(penalty) 하이퍼파라미터 값의 후보를 만든다.\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# 규제 하이퍼파라미터 값의 후보 범위 제작\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# 하이퍼파라미터 후보 딕셔너리 값 제작\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# 그리드 서치 객체 제작\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n",
    "\n",
    "# 그리드 서치 수행\n",
    "best_model = gridsearch.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "       5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "       3.59381366e+03, 1.00000000e+04])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "np.logspace(0, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "가장 좋은 패널티: l1\n가장 좋은 C값: 7.742636826811269\n"
     ]
    }
   ],
   "source": [
    "# 최선의 하이퍼파라미터 확인\n",
    "print('가장 좋은 패널티:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('가장 좋은 C값:', best_model.best_estimator_.get_params()['C'])\n"
   ]
  },
  {
   "source": [
    "* GridSearchCV : 최선의 하이퍼파라미터 확인한 후(교차검증에서 폴드 하나 제거하는 대신), 전체 데이터셋에서 최선의 하이퍼파라미터를 사용하여 모델 다시 훈련한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# 타깃 벡터 예측\n",
    "best_model.predict(features)"
   ]
  },
  {
   "source": [
    "## 12.2 랜덤 탐색을 사용해 최선의 모델 선택하기\n",
    "* 사이킷런 RandomizedSearchCV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 데이터 로드\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 로지스틱\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Camdidate Regularization penalty hyperparameter values\n",
    "penalty = ['l1','l2']\n",
    "\n",
    "# Create distn of candidate regularization\n",
    "C = uniform(loc=0, scale=4)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create randomized search\n",
    "randomizedsearch = RandomizedSearchCV(\n",
    "    logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit RS\n",
    "best_model = randomizedsearch.fit(features, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.51527429, 2.17259286, 2.43949273, 1.65416853, 0.3004436 ,\n",
       "       2.92883275, 2.39942691, 0.32694419, 3.5724987 , 2.48569169])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Define a uniform distn btw 0 and 4, sample 10 values\n",
    "uniform(loc=0, scale=4).rvs(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Penalty: l2\nBest C: 3.730229437354635\n"
     ]
    }
   ],
   "source": [
    "# best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "best_model.predict(features)"
   ]
  },
  {
   "source": [
    "## 12.3 Selecting Best Models from Multiple Learning Algorithms\n",
    "\n",
    "* Dictionary of candidate learning algorithms and their hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# load the data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# create a pipeline\n",
    "pipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n",
    "\n",
    "# create dictionary with candidate learning algorithms and their hyperparameters\n",
    "search_space = [{\"classifier\": [LogisticRegression()],\n",
    "\"classifier__penalty\": ['l1', 'l2'],\n",
    "\"classifier__C\": np.logspace(0,4,10)},\n",
    "{\"classifier\":[RandomForestClassifier()],\n",
    "\"classifier__n_estimators\": [10,100,1000],\n",
    "\"classifier__max_features\":[1,2,3]}]\n",
    "\n",
    "# Create grid search\n",
    "gridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(C=7.742636826811269, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# To view the best model\n",
    "best_model.best_estimator_.get_params()['classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Predict target vector\n",
    "best_model.predict(features)"
   ]
  },
  {
   "source": [
    "## 12.4 Selecting Best Models when preprocessing\n",
    "* Preprocessing step and parameters : make a pipeline including!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load Data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# create a preprocessing object that includes StandardScaler features and PCA\n",
    "preprocess = FeatureUnion([(\"std\", StandardScaler()), (\"pca\", PCA())])\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([(\"preprocess\",preprocess), (\"classifier\", LogisticRegression())])\n",
    "\n",
    "# Create space of candidate values\n",
    "search_space = [{\"preprocess__pca__n_components\": [1,2,3],\n",
    "\"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "\"classifier__C\": np.logspace(0,4,10)}]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(C=7.742636826811269, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "best_model.best_estimator_.get_params()[\"classifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=None,\n",
       "             transformer_list=[('std',\n",
       "                                StandardScaler(copy=True, with_mean=True,\n",
       "                                               with_std=True)),\n",
       "                               ('pca',\n",
       "                                PCA(copy=True, iterated_power='auto',\n",
       "                                    n_components=2, random_state=None,\n",
       "                                    svd_solver='auto', tol=0.0,\n",
       "                                    whiten=False))],\n",
       "             transformer_weights=None, verbose=False)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "best_model.best_estimator_.get_params()[\"preprocess\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# See the best n_components\n",
    "best_model.best_estimator_.get_params()[\"preprocess__pca__n_components\"]"
   ]
  },
  {
   "source": [
    "## 12.5 Speeding up model selection with parallelization\n",
    "* n_jobs = -1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 7330 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:   15.8s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load Data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# range of candidate regularization penalty hyperparameter values\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "\n",
    "# Create range of candidate values for C\n",
    "C = np.logspace(0, 4, 1000)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# grid search\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# fit grid search\n",
    "best_model = gridsearch.fit(features, target)"
   ]
  },
  {
   "source": [
    "## 12.6 Speeding Up Model Selection Using Algorithm-Specific Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=100, class_weight=None, cv='warn', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=100, multi_class='warn', n_jobs=None,\n",
       "                     penalty='l2', random_state=None, refit=True, scoring=None,\n",
       "                     solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# 사이킷런 활용하라!\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create cross-validated logistic regression\n",
    "logit = linear_model.LogisticRegressionCV(Cs=100) # 사이킷런 자체 내에 있는 model-specific cross-validation hyperparameter tuning 활용\n",
    "\n",
    "# Train model\n",
    "logit.fit(features, target)"
   ]
  },
  {
   "source": [
    "* 단점 : LogisticRegressionCV는 오직 C 파라미터만 조정 가능하다.\n",
    "    * penalty는 조정할 수 없다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 12. Evaluating Performance After Model Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* model selection을 통해 모델 성능을 평가하고 싶다.\n",
    "* biased evaluation을 피하고자 nested cross-validation을 활용한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9534313725490197"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# range of 20 candidate for C\n",
    "C = np.logspace(0, 4, 20)\n",
    "\n",
    "hyperparameters = dict(C=C)\n",
    "\n",
    "# grid search\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "# nested cross-validation and output the average score\n",
    "cross_val_score(gridsearch, features, target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# run the gridsearch and fitting and it is our inner cross-validation to find the best model\n",
    "best_model = gridsearch.fit(features, target)"
   ]
  },
  {
   "source": [
    "* inner cross-validation : GridSearchCV object\n",
    "* wrap in an outer cross-validation : cross_val_score\n",
    "    * trained 20 candidate models * 5 times = 100 models\n",
    "    * nest clf inside a new cross-validation(3 folds, default)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(gridsearch, features, target)"
   ]
  },
  {
   "source": [
    "* inner cross-validation trained 20 models * 5 times to find the best models\n",
    "* The model evaluated by an outer 3-fold cv with total of 300 models trained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.3 64-bit (conda)",
   "display_name": "Python 3.7.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "e134e05457d34029b6460cd73bbf1ed73f339b5b6d98c95be70b69eba114fe95"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}