{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit (conda)",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "be616a5aca28d3db9b0a9b4d7f249131c3d6f9e1a336d502c86d6e4c207752b0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 16. Logistic Regression\n",
    "\n",
    "## 16.0 Introduction\n",
    "\n",
    "## 16.1 Training a Binary Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* Simple classifier model\n",
    "* 사이킷런의 LogisticRegression 으로 로지스틱 회귀를 훈련한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/statstics/anaconda3/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:100, :]\n",
    "target = iris.target[:100]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# logistic regression object\n",
    "logistic_regression = LogisticRegression(random_state=0)\n",
    "\n",
    "# Train model\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "source": [
    "* 로지스틱 회귀는 이진 분류기이다.\n",
    "* 선형 모델로 로지스틱 함수(sigmoid function)에 포함된다.\n",
    "\n",
    "$$P(y_i = 1 | X) = {{1}\\over {1+e^{-(\\beta_0 + \\beta_1 x)}}}$$\n",
    "\n",
    "* 로지스틱 함수로, 0과 1 사이 값으로 결과를 제한하여 확률로 해석되게 한다.\n",
    "    * 0.5 를 넘으면 class 1이고, 아니면 class 0 으로 예측한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "new_observation = [[.5,.5,.5,.5]]\n",
    "\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.18944274, 0.81055726]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# View predicted probabilities\n",
    "model.predict_proba(new_observation)"
   ]
  },
  {
   "source": [
    "## 16.2 Training a Multiclass Classifier\n",
    "\n",
    "* 2 클래스 이상에서 분류기 모형으로 훈련한다.\n",
    "* 로지스틱회귀로 1대 나머지를 활용하거나 multinomial 방법을 이용한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# one-vs-rest 로지스틱 회귀 객체를 만든다. multi_class=\"ovr\" 이 핵심이다.\n",
    "logistic_regression = LogisticRegression(random_state=0, multi_class=\"ovr\")\n",
    "\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "source": [
    "$$P(y_i = k | X) = {{e^{\\beta_k x_i}} \\over {\\sum_{j=1}^K e^{\\beta_j x_i}}}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* LogisticRegression\n",
    "    * One-vs-Rest(binary) : ovr for OVR(default)\n",
    "    * MLR by setting to multinomial"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 16.3 Reducing Variance Through Regularization\n",
    "* Tune the Regularization strength hyperparameter C"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# iris\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Standardized features :: 로지스틱 회귀할 때 표준화를 해준다.\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# Create decision tree regression object\n",
    "logistic_regression = LogisticRegressionCV(\n",
    "    penalty='l2', Cs=10, random_state=0, n_jobs=-1)\n",
    "\n",
    "# Train model\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "source": [
    "* Regularization : 분산을 줄이기 위해 복잡한 모델에 패널티를 주는 방식이다.\n",
    "    * Penaly term이 최소화하고자 하는 손실 함수에 더해진다.\n",
    "    * 전형적으로 L1, L2 패널티를 사용한다.\n",
    "* L1 패널티\n",
    "$$\\alpha \\sum_{j=1}^p |\\hat{\\beta}_j|$$\n",
    "\n",
    " * $\\hat{\\beta}_j$ : 학습된 p개의 피처들의 j번째 파라미터\n",
    " * $\\alpha$ : 정규화 강도를 표기한 하이퍼파라미터\n",
    "\n",
    "* L2 패널티\n",
    "$$\\alpha \\sum_{j=1}^p \\hat{\\beta}^2_j$$\n",
    "\n",
    "* $\\alpha$ 큰 값은 더 큰 파라미터 값에 대해 피널티를 증가시킨다.\n",
    "* 사이킷런에서는 C를 $\\alpha$ 대신 사용하며, 정규화 강도의 역순이 된다.  \n",
    "* $C = {1\\over \\alpha}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 16.4 Training a Classifier on Very Large Data\n",
    "\n",
    "* simple classifier model on very large dataset\n",
    "    * logistic regression using the stochastic average gradient(SAG) solver(사이킷런의 함수, 파라미터)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# 로지스틱 회귀 객체 생성\n",
    "logistic_regression = LogisticRegression(random_state=0, solver=\"sag\")\n",
    "\n",
    "# 모델 훈련\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "source": [
    "* 사이킷런의 로지스틱 회귀 함수가 로지스틱 회귀를 훈련시키기 위한 많은 테크닉을 제공한다.\n",
    "    * 이를 solver 라고 한다.\n",
    "    * 자동적으로 최상의 solver를 선택했고, solver를 위해 우리가 할 필요가 없다. 그러나 한 가지는 알고 가자.\n",
    "* Stochastic average gradient descent\n",
    "    * 데이터가 매우 클 때, 특히 다른 solver보다 훨씬 더 빠르게 모델을 훈련한다.\n",
    "    * 그러나 피처 스케일링에 매우 민감하므로, 표준화하는 게 특히 중요하다.\n",
    "    * 이 알고리즘은 solver = 'sag' 로 활용할 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 16.5 Handling Imbalanced Classes\n",
    "* Train a simple classifier model : 사이킷런의 LogisticRegression 활용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "features = features[40:, :]\n",
    "target = target[40:]\n",
    "\n",
    "# Create target vector if class 0, 나머지는 모두 1로 변경한다.\n",
    "target = np.where((target==0),0, 1)\n",
    "\n",
    "# 피처를 표준화한다.\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# Create Decision Tree regresssion object\n",
    "logistic_regression = LogisticRegression(random_state=0, class_weight=\"balanced\")\n",
    "\n",
    "# Train model\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "source": [
    "* 이 외에도 다양한 학습 알고리즘이 사이킷런에 있다.\n",
    "    * LogisticRegression : 내장된 방식으로 불균형 클래스를 처리한다.\n",
    "    * 매우 불균형 한 경우, 활용할 수 없다면, class_weight 파라미터를 활용한다.\n",
    "        * 이를 활용하면, 균형된 argument가 자동적으로 weigh class를 역으로 비율에 비례한다.\n",
    "\n",
    "$$w_j = {{n} \\over {kn_j}}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}