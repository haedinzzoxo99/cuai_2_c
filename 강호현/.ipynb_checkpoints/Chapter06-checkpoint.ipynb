{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. 텍스트 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 소개\n",
    "\n",
    "* 비정형(unstructured) 텍스트 데이터\n",
    "    * 책의 본문이나 트윗\n",
    "    * 텍스트를 풍부한 정보를 가진 특성으로 변환하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 텍스트 정제하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 비정형 텍스트 데이터에 기본적인 정제 작업 실시\n",
    "    * strip, replace, split 파이썬 기본 문자열 메서드로 텍스트 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\"  Interrobang. By Aishwarya Henriette  \",\n",
    "            \"Parking And Going. By Karl Gautier\",\n",
    "            \"   Today Is The night. By Jarek Prakash   \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공백 문자 제거\n",
    "strip_whitespace = [string.strip() for string in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By Aishwarya Henriette',\n",
       " 'Parking And Going. By Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마침표 제거\n",
    "remove_periods = [string.replace(\".\",\"\") for string in strip_whitespace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By Aishwarya Henriette',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 제작\n",
    "def capitalizer(string: str) -> str:\n",
    "        return string.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식\n",
    "import re\n",
    "\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\", \"X\", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텍스트 데이터 : 특성으로 만들기 전에 정제되어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 HTML 파싱과 정제하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 뷰티풀 수프(beautiful soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<div class='full_name'><span style='font-weight:bold'>\n",
    "Masego</span> Azra</div>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMasego Azra'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "soup.find(\"div\", { \"class\" : \"full_name\" }).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 구두점 삭제하기\n",
    "* 텍스트 데이터에서 구두점을 삭제하고 싶다.\n",
    "* 구두점 글자의 딕셔너리를 만들어 translate 메서드 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "text_data = ['Hi!!! I. Love. This. Song....',\n",
    "            '10000% Agree!!! #LoveIT',\n",
    "            'Right?!?!?!']\n",
    "\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                           if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* translate : 속도가 매우 빨라 인기 있는 파이썬 함수\n",
    "    * 유니코드 구두점을 키로, 값은 None인 punctuation 딕셔너리를 제작한다.\n",
    "    * punctuation 있는 모든 문자를 None 바꾸어 구두점 삭제 효과를 낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 텍스트 토큰화하기\n",
    "* 텍스트 개별 단어 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 파이썬 자연어 처리 툴킷 NLTK : 단어 토큰화를 비롯해 강력한 텍스트 처리 기능을 가진다.\n",
    "    * Natural Language TooklKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/statstics/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # 구두점 데이터 다운로드\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# text\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "word_tokenize(string) # 단어를 토큰으로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 나누기\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "string = \"The science of today is the technology of tomorrow. \\\n",
    "          Tomorrow is today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 토큰화 : 텍스트 데이터 정제 후에 여러번 수행하므로 유용한 특성을 만들기 위해 텍스트를 데이터로 변환하는 첫번째 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 불용어 삭제하기\n",
    "* 토큰화된 텍스트 데이터에서 정보가가 없는 단어를 삭제한다.(a, is, of, on)\n",
    "* NLTK의 stopwords 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/statstics/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 데이터\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_words = ['i','am','going', 'to', 'go','to','the','store','and','park']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 불용어(stop word) : 작업 전에 삭제할 단어 혹은 유용한 정보가 거의 없고 매우 자주 등장하는 단어가 된다.\n",
    "* NLTK가 불용어 리스트가 있으니 그 목록에서 확인 및 제거할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 확인\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NLTK의 stopwords는 토큰화된 단어는 소문자를 가정한다.\n",
    "* 사이킷런도 영어 불용어 리스트 제공한다.(318개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 179)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS \n",
    "\n",
    "len(ENGLISH_STOP_WORDS), len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사이킷런 불용어는 frozenset 객체 : 인덱스 사용 불가\n",
    "    * 리스트 변환 후 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['around', 'three', 'of', 'mine', 'thereafter']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ENGLISH_STOP_WORDS)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 어간 추출하기\n",
    "\n",
    "* 토큰으로 나눈 단어를 어간으로 바꾼다.\n",
    "* NLTK PorterStemmer 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# 단어 토큰 제작\n",
    "tokenized_words = ['i','am','humbled','by','this','traditional','meeting']\n",
    "\n",
    "# 어간 추출기 제작\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# 적용\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 어간 추출 : 단어의 어간을 구분하여 기본 의미를 유지하면서 어미 제거(~ing)\n",
    "    * 읽기는 힘들지만 기본 의미에 가까워지고 샘플 간 비교하기 더 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 품사 태깅하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사전 훈련된 NLTK 품사 태깅 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/statstics/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# 사전훈련된 품사 태깅을 사용한다.\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "text_tagged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 출력 : 단어와 품사 태그로 이루어진 튜플의 리스트\n",
    "* 펜 트리뱅크(penn Treebank) : 구문 주석 말뭉치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NNP 고유 명사, 단수  \n",
    "2. NN 명사, 단수 또는 불가산 명사\n",
    "3. RB 부사\n",
    "4. VBD 동사, 과거형\n",
    "5. VBG 동사, 동명사 또는 현재 분사\n",
    "6. JJ 형용사\n",
    "7. PRP 인칭 대명사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 태깅되면 태그를 이용해 특정 품사 찾을 수 있다.\n",
    "[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플의 트윗 문장\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "         \"Political science is an amazing field\",\n",
    "         \"San Francisco is an awesome city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes_ : 특성이 어떤 품사 갖는지 안다.\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 브라운 코퍼스 데이터 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 백오프 n-그램 태그 모델\n",
    "    * n : 한 단어의 품사를 예측하기 위해 고려할 이전 단어의 수\n",
    "    * TrigramTagger : 이전 두 단어 고려\n",
    "    * BigramTagger : 이전 한 단어 고려\n",
    "    * UnigramTagger : 그 단어 자체만 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/statstics/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 브라운코퍼스\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "sentences = brown.tagged_sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = sentences[:4000]\n",
    "test = sentences[4000:] # 나머지 623\n",
    "\n",
    "# 백오프 태그 객체 제작\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "\n",
    "# 정확도 확인\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한글 품사 태깅 : KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "text = '태양계는 지금으로부터 약 46억 년 전, 거대한 분자 구름의 일부분이 중력 붕괴를 일으키면서 형성되었다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('태양계', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('지금', 'Noun'),\n",
       " ('으로부터', 'Josa'),\n",
       " ('약', 'Noun'),\n",
       " ('46억', 'Number'),\n",
       " ('년', 'Noun'),\n",
       " ('전', 'Noun'),\n",
       " (',', 'Punctuation'),\n",
       " ('거대한', 'Adjective'),\n",
       " ('분자', 'Noun'),\n",
       " ('구름', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('일부분', 'Noun'),\n",
       " ('이', 'Josa'),\n",
       " ('중력', 'Noun'),\n",
       " ('붕괴', 'Noun'),\n",
       " ('를', 'Josa'),\n",
       " ('일으키면서', 'Verb'),\n",
       " ('형성', 'Noun'),\n",
       " ('되었다', 'Verb')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기존 태거도 활용 가능하다.\n",
    "* 형태소 추출 morphs 메서드\n",
    "* 명사만 추출 nouns 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['태양계',\n",
       " '는',\n",
       " '지금',\n",
       " '으로부터',\n",
       " '약',\n",
       " '46억',\n",
       " '년',\n",
       " '전',\n",
       " ',',\n",
       " '거대한',\n",
       " '분자',\n",
       " '구름',\n",
       " '의',\n",
       " '일부분',\n",
       " '이',\n",
       " '중력',\n",
       " '붕괴',\n",
       " '를',\n",
       " '일으키면서',\n",
       " '형성',\n",
       " '되었다']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['태양계', '지금', '약', '년', '전', '분자', '구름', '일부분', '중력', '붕괴', '형성']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.nouns(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 텍스트를 BoW로 인코딩하기\n",
    "* 사이킷런 : CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텍스트 데이터에서 특정 단어의 등장 횟수를 나타내는 특성 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                     'Sweden is best','Germany beats both'])\n",
    "\n",
    "# BoW 특성 행렬\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대용량 데이터에서는 희소 행렬 필수적\n",
    "# toarray() : 샘플의 단어  카운트 행렬 확인\n",
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_feature_names : 각 특성에 연결된 단어 확인 가능\n",
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BoW(Bag of Word) 모델\n",
    "    * 텍스트 데이터에 있는 고유한 단어마다 하나의 특성 제작\n",
    "    * 각 단어가 샘플에 등장한 횟수 포함\n",
    "    * 수천 개의 특성을 가진 행렬을 만들 수 있다.\n",
    "    * BoW 특성 행렬 : 데이터 저장 공간을 줄일 수 있다.\n",
    "        * 대부분 0이지만, 0 아닌 값만 저장한다. 희소 행렬이다.\n",
    "        * 메모리 줄인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CountVectorizer\n",
    "    * 기본적으로 희소 행렬 출력\n",
    "    * ngram_range : n-그램의 최소와 최대 크기 지정 가능\n",
    "    * stop_words : 내장된 리스트나 사용자가 지정한 리스트에 포함된 유용하지 않은 단어 제거 가능\n",
    "    * vocabulary : 대상 단어나 구를 제한 가능. 국가 일므만 담은 BoW 특성 행렬 등.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "                             stop_words='english',\n",
    "                             vocabulary=['brazil'])\n",
    "bag = count_2gram.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1그램과 2그램을 확인\n",
    "count_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 어휘 사전 : 텍스트에서 고유한 단어를 추출해서 순서대로 번호 매긴 것이다.\n",
    "    * vocabulary_ 속성에 딕셔너리로 저장된다.\n",
    "    * max_df : 단어가 등장할 문서의 최대 개수 지정(불용어 같은 너무 자주 등장하는 단어 제거)\n",
    "    * min_df : 단어가 등장할 문서의 최소 개수 지정(드문 단어 제거)\n",
    "        * 위 두 매개변수에 0~1사이 실숫값 지정하면 전체 문서 개수에 대한 비율이 된다.\n",
    "    * max_features : CountVectorizer 어휘 사전의 크기를 제한.\n",
    "        * 전체 문서에서 빈도순으로 최상위 max_features 개 단어가 추출된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9 단어 중요도에 가중치 부여하기\n",
    "* tf-idf(단어 빈도-역문서 빈도) : 샘플에서 단어의 중요도에 따라 가중치가 부여된 BoW 모델 필요시 활용\n",
    "    * 하나의 문서에 등장하는 단어의 빈도와 다른 모든 문서에 등장핳는 빈도를 비교한다.\n",
    "    * 사이킷런 TfidfVectorizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!','Sweden is best', 'Germany beats both'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf 특성 행렬 제작\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 희소 행렬로 출력되므로, 밀집 배열로 출력하려면 toarray 활용.\n",
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특성 이름 확인 : vocabulrary_\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한 문서에 많이 등장하면 그 문서에 더 중요한 단어이다.\n",
    "    * tf(term frequence) : 단어 빈도\n",
    "* 한 단어가 여러 문서에 나타나면 반대로 특정 문서에 중요한 것은 아니다.\n",
    "    * df(document frequence) : 문서 빈도\n",
    "* 두 통계치를 연결하여 tf를 idf(역문서 빈도)에 곱한다.\n",
    "    * tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tf-idf(t,d) = tf(t,d) \\times idf(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$idf(t) = \\log{{1+n_d}\\over{1+df(d,t)}}+1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사이킷런은 유클리드 노름(L2 노름)으로 tf-idf 벡터 정규화\n",
    "    * 결괏값이 높을수록 그 문서에서 더 중요한 단어이다.\n",
    "    * TfidfVectorizer::smooth_idf = True(기본값)이면 앞의 공식이 사용된다.\n",
    "    * 분모와 분자에 1을 더하여 모든 단어가 포함된 가상의 문서를 만들고, 분모의 0이 되는 것을 막는다.\n",
    "    * 반대롤 모든 문서에 포함된 단어가 있다면 분모가 매우 커져서 로그 값이 0이되므로, idf 공식에 1을 더한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* smooth_idf=False : 분모와 분자에 1을 더하지 않는 공식으로 변경한다.\n",
    "$$idf(t) = \\log{{n_d}\\over{df(d,t)}}+1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TfidfVectorizer::ngram_range, max_df, min_df, max_features 지원합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
